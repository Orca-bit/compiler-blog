---
title: ç¡¬ä»¶æ¶æ„æ·±å…¥
weight: 5
next: hardware/introduction
---

# ç¡¬ä»¶æ¶æ„æ·±å…¥

æ·±å…¥ç†è§£ç›®æ ‡ç¡¬ä»¶æ¶æ„æ˜¯ç¼–è¯‘å™¨ä¼˜åŒ–çš„åŸºç¡€ã€‚æœ¬ç« èŠ‚è¯¦ç»†åˆ†æGPUã€TPUã€FPGAç­‰ç°ä»£åŠ é€Ÿå™¨çš„ç¡¬ä»¶æ¶æ„ï¼Œä¸ºç¼–è¯‘å™¨è®¾è®¡æä¾›ç¡¬ä»¶å±‚é¢çš„æ´å¯Ÿã€‚

## ğŸ“‹ ç« èŠ‚æ¦‚è§ˆ

{{< cards >}}
  {{< card link="hardware/introduction" title="ç¡¬ä»¶æ¶æ„æ¦‚è¿°" icon="chip" subtitle="ç°ä»£è®¡ç®—æ¶æ„åˆ†ç±»" >}}
  {{< card link="hardware/gpu-arch" title="GPU æ¶æ„åˆ†æ" icon="chip" subtitle="å¹¶è¡Œè®¡ç®—æ¶æ„è¯¦è§£" >}}
  {{< card link="hardware/ai-accelerators" title="AI åŠ é€Ÿå™¨" icon="chip" subtitle="TPUã€NPU æ¶æ„åˆ†æ" >}}
  {{< card link="hardware/fpga" title="FPGA æ¶æ„" icon="server" subtitle="å¯é‡æ„è®¡ç®—å¹³å°" >}}
  {{< card link="hardware/memory-systems" title="å†…å­˜ç³»ç»Ÿ" icon="server" subtitle="å­˜å‚¨å±‚æ¬¡ä¸ä¼˜åŒ–" >}}
  {{< card link="hardware/interconnect" title="äº’è¿ç½‘ç»œ" icon="globe" subtitle="ç‰‡ä¸Šç½‘ç»œä¸é€šä¿¡" >}}
{{< /cards >}}

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« èŠ‚çš„å­¦ä¹ ï¼Œä½ å°†æŒæ¡ï¼š

- **æ¶æ„åˆ†ç±»**: ç†è§£ä¸åŒç±»å‹å¤„ç†å™¨çš„æ¶æ„ç‰¹ç‚¹
- **æ€§èƒ½æ¨¡å‹**: å»ºç«‹ç¡¬ä»¶æ€§èƒ½åˆ†æçš„ç†è®ºåŸºç¡€
- **ä¼˜åŒ–ç­–ç•¥**: é’ˆå¯¹ç‰¹å®šç¡¬ä»¶çš„ç¼–è¯‘å™¨ä¼˜åŒ–æ–¹æ³•
- **è®¾è®¡æƒè¡¡**: ç†è§£ç¡¬ä»¶è®¾è®¡ä¸­çš„æ€§èƒ½ä¸åŠŸè€—æƒè¡¡
- **å‘å±•è¶‹åŠ¿**: äº†è§£ç¡¬ä»¶æ¶æ„çš„å‘å±•æ–¹å‘

## ğŸ—ï¸ ç°ä»£è®¡ç®—æ¶æ„åˆ†ç±»

### æŒ‰å¹¶è¡Œåº¦åˆ†ç±»
```
è®¡ç®—æ¶æ„
â”œâ”€â”€ æ ‡é‡å¤„ç†å™¨ (Scalar)
â”‚   â””â”€â”€ ä¼ ç»Ÿ CPU æ ¸å¿ƒ
â”œâ”€â”€ å‘é‡å¤„ç†å™¨ (Vector)
â”‚   â””â”€â”€ SIMD å•å…ƒ
â”œâ”€â”€ å¹¶è¡Œå¤„ç†å™¨ (Parallel)
â”‚   â”œâ”€â”€ å¤šæ ¸ CPU
â”‚   â”œâ”€â”€ GPU (SIMT)
â”‚   â””â”€â”€ ä¼—æ ¸å¤„ç†å™¨
â””â”€â”€ ä¸“ç”¨å¤„ç†å™¨ (Specialized)
    â”œâ”€â”€ DSP
    â”œâ”€â”€ AI åŠ é€Ÿå™¨
    â””â”€â”€ ASIC
```

### æŒ‰å­˜å‚¨è®¿é—®æ¨¡å¼åˆ†ç±»
```
å­˜å‚¨æ¶æ„
â”œâ”€â”€ å†¯Â·è¯ºä¾æ›¼æ¶æ„
â”‚   â””â”€â”€ æŒ‡ä»¤å’Œæ•°æ®å…±äº«å­˜å‚¨
â”œâ”€â”€ å“ˆä½›æ¶æ„
â”‚   â””â”€â”€ æŒ‡ä»¤å’Œæ•°æ®åˆ†ç¦»å­˜å‚¨
â”œâ”€â”€ æ•°æ®æµæ¶æ„
â”‚   â””â”€â”€ æ•°æ®é©±åŠ¨è®¡ç®—
â””â”€â”€ è¿‘å­˜å‚¨è®¡ç®—
    â””â”€â”€ Processing-in-Memory
```

## ğŸ–¥ï¸ GPU æ¶æ„æ·±å…¥åˆ†æ

### NVIDIA GPU æ¶æ„æ¼”è¿›
```
æ¶æ„æ¼”è¿›æ—¶é—´çº¿:
Fermi (2010) â†’ Kepler (2012) â†’ Maxwell (2014) â†’ Pascal (2016) 
â†’ Volta (2017) â†’ Turing (2018) â†’ Ampere (2020) â†’ Ada Lovelace (2022) â†’ Hopper (2022)
```

### GPU å±‚æ¬¡ç»“æ„
```
GPU èŠ¯ç‰‡
â”œâ”€â”€ GPC (Graphics Processing Cluster)
â”‚   â”œâ”€â”€ TPC (Texture Processing Cluster)
â”‚   â”‚   â””â”€â”€ SM (Streaming Multiprocessor)
â”‚   â”‚       â”œâ”€â”€ CUDA Cores
â”‚   â”‚       â”œâ”€â”€ Tensor Cores (Volta+)
â”‚   â”‚       â”œâ”€â”€ RT Cores (Turing+)
â”‚   â”‚       â”œâ”€â”€ Shared Memory
â”‚   â”‚       â”œâ”€â”€ Register File
â”‚   â”‚       â””â”€â”€ Warp Scheduler
â”‚   â””â”€â”€ Raster Engine
â”œâ”€â”€ Memory Controller
â”œâ”€â”€ L2 Cache
â””â”€â”€ NVLink/PCIe Interface
```

### SM (Streaming Multiprocessor) è¯¦ç»†ç»“æ„
```
SM å†…éƒ¨ç»„ç»‡ (Ampere GA102):
â”œâ”€â”€ 128 CUDA Cores (FP32)
â”œâ”€â”€ 64 CUDA Cores (INT32)
â”œâ”€â”€ 4 Third-gen Tensor Cores
â”œâ”€â”€ 1 RT Core (2nd gen)
â”œâ”€â”€ 4 Warp Schedulers
â”œâ”€â”€ 8 Dispatch Units
â”œâ”€â”€ 128 KB Shared Memory/L1 Cache
â”œâ”€â”€ 65536 x 32-bit Registers
â””â”€â”€ 16 Load/Store Units
```

## ğŸ§  AI åŠ é€Ÿå™¨æ¶æ„

### Google TPU æ¶æ„
```
TPU v4 æ¶æ„:
â”œâ”€â”€ Matrix Multiply Unit (MXU)
â”‚   â””â”€â”€ 128x128 systolic array
â”œâ”€â”€ Vector Processing Unit (VPU)
â”‚   â”œâ”€â”€ Vector ALUs
â”‚   â”œâ”€â”€ Vector Registers
â”‚   â””â”€â”€ Scalar Unit
â”œâ”€â”€ High Bandwidth Memory (HBM)
â”‚   â””â”€â”€ 32 GB HBM2
â”œâ”€â”€ Interconnect
â”‚   â””â”€â”€ 2D Torus Network
â””â”€â”€ Host Interface
    â””â”€â”€ PCIe Gen4
```

### è„‰åŠ¨é˜µåˆ— (Systolic Array) åŸç†
```
è„‰åŠ¨é˜µåˆ—è®¡ç®—æ¨¡å¼:

è¾“å…¥ A â†’  [PE] â†’ [PE] â†’ [PE] â†’ è¾“å‡º
         â†“     â†“     â†“
è¾“å…¥ B â†’ [PE] â†’ [PE] â†’ [PE] â†’ è¾“å‡º
         â†“     â†“     â†“  
è¾“å…¥ B â†’ [PE] â†’ [PE] â†’ [PE] â†’ è¾“å‡º
         â†“     â†“     â†“
        è¾“å‡º  è¾“å‡º  è¾“å‡º

PE (Processing Element):
â”œâ”€â”€ ä¹˜æ³•å™¨
â”œâ”€â”€ åŠ æ³•å™¨
â”œâ”€â”€ ç´¯åŠ å™¨
â””â”€â”€ å¯„å­˜å™¨
```

### åä¸ºæ˜‡è…¾ NPU æ¶æ„
```
æ˜‡è…¾ 910 æ¶æ„:
â”œâ”€â”€ AI Core
â”‚   â”œâ”€â”€ Cube Unit (çŸ©é˜µè®¡ç®—)
â”‚   â”œâ”€â”€ Vector Unit (å‘é‡è®¡ç®—)
â”‚   â”œâ”€â”€ Scalar Unit (æ ‡é‡è®¡ç®—)
â”‚   â””â”€â”€ Local Memory
â”œâ”€â”€ AI CPU
â”‚   â””â”€â”€ ARM Cortex-A55
â”œâ”€â”€ DVPP (Digital Vision Pre-Processing)
â”œâ”€â”€ HBM2e Memory
â””â”€â”€ PCIe 4.0 Interface
```

## ğŸ”§ FPGA æ¶æ„åˆ†æ

### Xilinx FPGA æ¶æ„
```
Xilinx Versal ACAP:
â”œâ”€â”€ Programmable Logic (PL)
â”‚   â”œâ”€â”€ CLB (Configurable Logic Block)
â”‚   â”œâ”€â”€ DSP Slices
â”‚   â”œâ”€â”€ Block RAM
â”‚   â””â”€â”€ UltraRAM
â”œâ”€â”€ Processing System (PS)
â”‚   â”œâ”€â”€ ARM Cortex-A72 (Dual-core)
â”‚   â”œâ”€â”€ ARM Cortex-R5F (Dual-core)
â”‚   â””â”€â”€ DDR4/LPDDR4 Controller
â”œâ”€â”€ AI Engine Array
â”‚   â”œâ”€â”€ AI Engine Tiles
â”‚   â”œâ”€â”€ Memory Tiles
â”‚   â””â”€â”€ Interface Tiles
â””â”€â”€ Hardened IP
    â”œâ”€â”€ PCIe Gen4
    â”œâ”€â”€ Ethernet
    â””â”€â”€ Interlaken
```

### CLB (Configurable Logic Block) ç»“æ„
```
CLB å†…éƒ¨ç»„ç»‡:
â”œâ”€â”€ 8 x LUT6 (6-input Look-Up Table)
â”œâ”€â”€ 16 x Flip-Flops
â”œâ”€â”€ 2 x Distributed RAM
â”œâ”€â”€ 2 x SRL (Shift Register LUT)
â”œâ”€â”€ Carry Logic
â””â”€â”€ Multiplexers
```

### AI Engine æ¶æ„
```
AI Engine Tile:
â”œâ”€â”€ VLIW Vector Processor
â”‚   â”œâ”€â”€ Vector ALU
â”‚   â”œâ”€â”€ Scalar ALU
â”‚   â””â”€â”€ Load/Store Unit
â”œâ”€â”€ Program Memory (16 KB)
â”œâ”€â”€ Data Memory (32 KB)
â”œâ”€â”€ AXI4-Stream Interface
â””â”€â”€ Cascade Connections
```

## ğŸ’¾ å†…å­˜ç³»ç»Ÿæ¶æ„

### GPU å†…å­˜å±‚æ¬¡
```
GPU å†…å­˜å±‚æ¬¡ (å»¶è¿Ÿ/å¸¦å®½):

å¯„å­˜å™¨ (Registers)
â”œâ”€â”€ å»¶è¿Ÿ: 1 cycle
â”œâ”€â”€ å¸¦å®½: ~20 TB/s
â””â”€â”€ å®¹é‡: 64KB per SM

å…±äº«å†…å­˜ (Shared Memory)
â”œâ”€â”€ å»¶è¿Ÿ: ~20 cycles
â”œâ”€â”€ å¸¦å®½: ~19 TB/s
â””â”€â”€ å®¹é‡: 128KB per SM

L1 ç¼“å­˜
â”œâ”€â”€ å»¶è¿Ÿ: ~80 cycles
â”œâ”€â”€ å¸¦å®½: ~9 TB/s
â””â”€â”€ å®¹é‡: 128KB per SM

L2 ç¼“å­˜
â”œâ”€â”€ å»¶è¿Ÿ: ~200 cycles
â”œâ”€â”€ å¸¦å®½: ~7 TB/s
â””â”€â”€ å®¹é‡: 40MB (A100)

å…¨å±€å†…å­˜ (HBM2e)
â”œâ”€â”€ å»¶è¿Ÿ: ~300 cycles
â”œâ”€â”€ å¸¦å®½: ~2 TB/s
â””â”€â”€ å®¹é‡: 80GB (A100)
```

### HBM (High Bandwidth Memory) æ¶æ„
```
HBM2e å †æ ˆç»“æ„:
â”œâ”€â”€ Logic Die (åº•å±‚)
â”‚   â”œâ”€â”€ PHY Interface
â”‚   â”œâ”€â”€ Memory Controller
â”‚   â””â”€â”€ TSV (Through-Silicon Via)
â”œâ”€â”€ DRAM Dies (8å±‚)
â”‚   â”œâ”€â”€ Bank Groups
â”‚   â”œâ”€â”€ Banks
â”‚   â””â”€â”€ Subarrays
â””â”€â”€ Microbumps
    â””â”€â”€ 1024-bit Interface

æ€§èƒ½ç‰¹æ€§:
â”œâ”€â”€ å¸¦å®½: 460 GB/s per stack
â”œâ”€â”€ å®¹é‡: 16/24/32 GB per stack
â”œâ”€â”€ åŠŸè€—: ~5W per stack
â””â”€â”€ å»¶è¿Ÿ: ~100ns
```

## ğŸŒ ç‰‡ä¸Šç½‘ç»œ (NoC) æ¶æ„

### 2D Mesh ç½‘ç»œ
```
2D Mesh æ‹“æ‰‘:
[R]â”€[R]â”€[R]â”€[R]
 â”‚   â”‚   â”‚   â”‚
[R]â”€[R]â”€[R]â”€[R]
 â”‚   â”‚   â”‚   â”‚  
[R]â”€[R]â”€[R]â”€[R]
 â”‚   â”‚   â”‚   â”‚
[R]â”€[R]â”€[R]â”€[R]

R = Router
æ¯ä¸ª Router è¿æ¥:
â”œâ”€â”€ æœ¬åœ°å¤„ç†å•å…ƒ
â”œâ”€â”€ 4ä¸ªç›¸é‚» Router
â””â”€â”€ è·¯ç”±é€»è¾‘
```

### Torus ç½‘ç»œ
```
Torus æ‹“æ‰‘ (ç¯å½¢è¿æ¥):
[R]â”€[R]â”€[R]â”€[R]
 â”‚ â•² â”‚ â•± â”‚ â•² â”‚
[R]â”€[R]â”€[R]â”€[R]
 â”‚ â•± â”‚ â•² â”‚ â•± â”‚
[R]â”€[R]â”€[R]â”€[R]
 â”‚ â•² â”‚ â•± â”‚ â•² â”‚
[R]â”€[R]â”€[R]â”€[R]

ä¼˜åŠ¿:
â”œâ”€â”€ æ›´çŸ­çš„å¹³å‡è·¯å¾„
â”œâ”€â”€ æ›´å¥½çš„è´Ÿè½½å‡è¡¡
â””â”€â”€ æ›´é«˜çš„å®¹é”™æ€§
```

## ğŸ“Š æ€§èƒ½å»ºæ¨¡

### Roofline æ¨¡å‹
```
æ€§èƒ½ (FLOPS) = min(
    Peak Performance,
    Arithmetic Intensity Ã— Memory Bandwidth
)

å…¶ä¸­:
- Arithmetic Intensity = FLOPs / Bytes
- Memory Bandwidth = å†…å­˜å¸¦å®½
- Peak Performance = å³°å€¼è®¡ç®—æ€§èƒ½
```

### GPU æ€§èƒ½åˆ†æ
```cpp
// GPU æ€§èƒ½è®¡ç®—ç¤ºä¾‹
struct GPUPerf {
    float compute_throughput;  // TFLOPS
    float memory_bandwidth;    // TB/s
    int sm_count;             // SM æ•°é‡
    int cores_per_sm;         // æ¯ä¸ª SM çš„æ ¸å¿ƒæ•°
    float base_clock;         // åŸºç¡€æ—¶é’Ÿé¢‘ç‡ (GHz)
    float boost_clock;        // åŠ é€Ÿæ—¶é’Ÿé¢‘ç‡ (GHz)
};

// A100 è§„æ ¼
GPUPerf a100 = {
    .compute_throughput = 19.5,  // FP32
    .memory_bandwidth = 2.0,
    .sm_count = 108,
    .cores_per_sm = 64,
    .base_clock = 1.41,
    .boost_clock = 1.73
};

// ç†è®ºå³°å€¼æ€§èƒ½è®¡ç®—
float peak_flops = a100.sm_count * a100.cores_per_sm * 
                   a100.boost_clock * 2; // 2 ops per cycle
```

## ğŸ”® æœªæ¥å‘å±•è¶‹åŠ¿

### æ–°å…´æ¶æ„
```
å‘å±•æ–¹å‘:
â”œâ”€â”€ è¿‘å­˜å‚¨è®¡ç®— (Near-Data Computing)
â”‚   â”œâ”€â”€ Processing-in-Memory (PIM)
â”‚   â”œâ”€â”€ Computational Storage
â”‚   â””â”€â”€ Smart Memory
â”œâ”€â”€ ç¥ç»å½¢æ€è®¡ç®— (Neuromorphic)
â”‚   â”œâ”€â”€ Spiking Neural Networks
â”‚   â”œâ”€â”€ Memristor Arrays
â”‚   â””â”€â”€ Event-driven Processing
â”œâ”€â”€ é‡å­è®¡ç®— (Quantum)
â”‚   â”œâ”€â”€ é‡å­æ¯”ç‰¹
â”‚   â”œâ”€â”€ é‡å­é—¨
â”‚   â””â”€â”€ é‡å­çº é”™
â””â”€â”€ å…‰å­¦è®¡ç®— (Optical)
    â”œâ”€â”€ å…‰å­å¤„ç†å™¨
    â”œâ”€â”€ å…‰å­¦äº’è¿
    â””â”€â”€ å…‰ç”µæ··åˆ
```

### æŠ€æœ¯æŒ‘æˆ˜
```
ä¸»è¦æŒ‘æˆ˜:
â”œâ”€â”€ åŠŸè€—å¢™ (Power Wall)
â”œâ”€â”€ å†…å­˜å¢™ (Memory Wall)
â”œâ”€â”€ æŒ‡ä»¤çº§å¹¶è¡Œå¢™ (ILP Wall)
â”œâ”€â”€ å¯é æ€§æŒ‘æˆ˜
â”œâ”€â”€ è®¾è®¡å¤æ‚åº¦
â””â”€â”€ åˆ¶é€ æˆæœ¬
```

## ğŸ”— ç›¸å…³èµ„æº

- [NVIDIA GPU æ¶æ„ç™½çš®ä¹¦](https://developer.nvidia.com/gpu-architecture)
- [AMD GPU æ¶æ„æ–‡æ¡£](https://www.amd.com/en/graphics/rdna-architecture)
- [Intel GPU æ¶æ„æŒ‡å—](https://www.intel.com/content/www/us/en/docs/graphics-for-linux/developer-guide/current/overview.html)
- [Google TPU è®ºæ–‡é›†](https://cloud.google.com/tpu/docs/system-architecture)
- [Xilinx FPGA æ¶æ„æ‰‹å†Œ](https://www.xilinx.com/support/documentation/user_guides/ug574-ultrascale-clb.pdf)
- [ARM å¤„ç†å™¨æ¶æ„å‚è€ƒ](https://developer.arm.com/documentation/ddi0487/latest/)